{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checklist.chk  consolidated.00.pth  params.json  tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/checkpoints/Llama3.2-3B-Instruct/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1a8c6bb326895960fb83ce099c359ac6  ./consolidated.00.pth\n",
      "8a395466b2ee225add24d8f3f7a155c5  ./orig_params.json\n",
      "8a395466b2ee225add24d8f3f7a155c5  ./params.json\n",
      "08292403f8b173e7524d7fba7bbbd2d3  ./tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/checkpoints/Llama3.2-3B-Instruct/checklist.chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Collecting torch\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/torch/2.4.1/torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m797.1/797.1 MB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting transformers\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/transformers/4.45.1/transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.9/9.9 MB\u001B[0m \u001B[31m86.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/sympy/1.13.3/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.2/6.2 MB\u001B[0m \u001B[31m100.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting networkx (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/networkx/3.3/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m122.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/fsspec/2024.9.0/fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m179.3/179.3 kB\u001B[0m \u001B[31m60.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cuda-nvrtc-cu12/12.1.105/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m102.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cuda-runtime-cu12/12.1.105/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m102.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cuda-cupti-cu12/12.1.105/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m110.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cudnn-cu12/9.1.0.70/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m12.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cublas-cu12/12.1.3.1/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m18.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cufft-cu12/11.0.2.54/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m48.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-curand-cu12/10.3.2.106/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m75.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cusolver-cu12/11.4.5.107/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m50.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cusparse-cu12/12.1.0.106/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m36.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-nccl-cu12/2.20.5/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m176.2/176.2 MB\u001B[0m \u001B[31m41.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-nvtx-cu12/12.1.105/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m34.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting triton==3.0.0 (from torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/triton/3.0.0/triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m209.4/209.4 MB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-nvjitlink-cu12/12.6.68/nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19.7/19.7 MB\u001B[0m \u001B[31m41.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/huggingface-hub/0.25.1/huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m436.4/436.4 kB\u001B[0m \u001B[31m46.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/regex/2024.9.11/regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m792.8/792.8 kB\u001B[0m \u001B[31m37.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/safetensors/0.4.5/safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m435.0/435.0 kB\u001B[0m \u001B[31m54.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/tokenizers/0.20.0/tokenizers-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.9/2.9 MB\u001B[0m \u001B[31m111.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/mpmath/1.3.0/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m49.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: mpmath, triton, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch\n",
      "Successfully installed fsspec-2024.9.0 huggingface-hub-0.25.1 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 regex-2024.9.11 safetensors-0.4.5 sympy-1.13.3 tokenizers-0.20.0 torch-2.4.1 transformers-4.45.1 triton-3.0.0\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Collecting sentencepiece\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/sentencepiece/0.2.0/sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m48.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch transformers\n",
    "# !pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.1)\n",
      "Collecting fairscale\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/fairscale/0.4.13/fairscale-0.4.13.tar.gz (266 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m266.3/266.3 kB\u001B[0m \u001B[31m19.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n",
      "\u001B[?25h  Installing backend dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting fire\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/fire/0.6.0/fire-0.6.0.tar.gz (88 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.4/88.4 kB\u001B[0m \u001B[31m9.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting blobfile\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/blobfile/3.0.0/blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.4/75.4 kB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.11/site-packages (from fairscale) (1.26.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor (from fire)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/termcolor/2.4.0/termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/pycryptodomex/3.20.0/pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m58.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: urllib3<3,>=1.25.3 in /opt/conda/lib/python3.11/site-packages (from blobfile) (2.0.7)\n",
      "Requirement already satisfied: lxml>=4.9 in /opt/conda/lib/python3.11/site-packages (from blobfile) (5.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Building wheels for collected packages: fairscale, fire\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332106 sha256=2a79a237425268727758f11685f800d9d51e51da44f63ca5fefbcc7455782fba\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/08/0b/32/a69926b6b7095a921c0743ffd72b8af62d4beb296292a3ad95\n",
      "  Building wheel for fire (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117033 sha256=68abae31348f824d74abbbe521cde54fb04782f712d254c3dcda4d2f7c71624c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/4e/23/87/814fecf44d243063139294004682a5371ae35c41600da58b90\n",
      "Successfully built fairscale fire\n",
      "Installing collected packages: termcolor, pycryptodomex, fire, blobfile, fairscale\n",
      "Successfully installed blobfile-3.0.0 fairscale-0.4.13 fire-0.6.0 pycryptodomex-3.20.0 termcolor-2.4.0\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch fairscale fire blobfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Collecting llama-models\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/llama-models/0.0.36/llama_models-0.0.36-py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m20.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from llama-models) (6.0.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from llama-models) (3.1.2)\n",
      "Collecting tiktoken (from llama-models)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/tiktoken/0.7.0/tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m99.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pydantic (from llama-models)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/pydantic/2.9.2/pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m434.9/434.9 kB\u001B[0m \u001B[31m92.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from llama-models) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->llama-models) (2.1.3)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->llama-models)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/annotated-types/0.7.0/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic->llama-models)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/pydantic-core/2.23.4/pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m101.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic->llama-models) (4.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken->llama-models) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken->llama-models) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->llama-models) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->llama-models) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->llama-models) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->llama-models) (2023.7.22)\n",
      "Installing collected packages: pydantic-core, annotated-types, tiktoken, pydantic, llama-models\n",
      "Successfully installed annotated-types-0.7.0 llama-models-0.0.36 pydantic-2.9.2 pydantic-core-2.23.4 tiktoken-0.7.0\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "drwxrwxrwx 2 root root 4.0K Sep 27 19:06 .\n",
      "drwxr-xr-x 1 root root 4.0K Sep 28 08:29 ..\n"
     ]
    }
   ],
   "source": [
    "!ls -alh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama-models'...\n",
      "remote: Enumerating objects: 748, done.\u001B[K\n",
      "remote: Counting objects: 100% (519/519), done.\u001B[K\n",
      "remote: Compressing objects: 100% (277/277), done.\u001B[K\n",
      "remote: Total 748 (delta 317), reused 344 (delta 222), pack-reused 229 (from 1)\u001B[K\n",
      "Receiving objects: 100% (748/748), 2.13 MiB | 776.00 KiB/s, done.\n",
      "Resolving deltas: 100% (424/424), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/meta-llama/llama-models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-models\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "/bin/bash: line 1: export: `models/scripts/example_chat_completion.py': not a valid identifier\n"
     ]
    }
   ],
   "source": [
    "!export CHECKPOINT_DIR=\n",
    "!export PYTHONPATH=$(git rev-parse --show-toplevel) torchrun models/scripts/example_chat_completion.py $CHECKPOINT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export RANK=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (0.25.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub) (2023.7.22)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "CHECKPOINT_DIR=/tmp/checkpoints/Llama3.2-3B-Instruct/tokenizer.model\n",
    "PYTHONPATH=$(git rev-parse --show-toplevel) torchrun models/scripts/example_chat_completion.py $CHECKPOINT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[?25lpulling manifest ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gpulling manifest \u001B[?25h\n",
      "Error: Head \"https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/dd/dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20240928%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20240928T095051Z&X-Amz-Expires=1200&X-Amz-SignedHeaders=host&X-Amz-Signature=07e792424b01d931e65660328000408fbd16731581440719e9894891a73fd3d2\": Forbidden\n"
     ]
    }
   ],
   "source": [
    "!ollama run llama3.2 \"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m tokenizer_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/checkpoints/Llama3.2-3B-Instruct/tokenizer.model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Инициализируем генератор текста\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m generator \u001B[38;5;241m=\u001B[39m \u001B[43mLlama\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mckpt_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/tmp/checkpoints/Llama3.2-3B-Instruct/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_seq_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_seq_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_parallel_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_parallel_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Примеры запросов для генерации текста\u001B[39;00m\n\u001B[1;32m     29\u001B[0m prompts \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe color of the sky is blue but sometimes it can also be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     31\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\\\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mba ba black sheep, have you any wool?\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     37\u001B[0m ]\n",
      "File \u001B[0;32m/workdir/models/llama3/reference_impl/generation.py:103\u001B[0m, in \u001B[0;36mLlama.build\u001B[0;34m(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;124;03mBuild a Llama instance by initializing and loading a model checkpoint.\u001B[39;00m\n\u001B[1;32m     80\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03m    and loads the pre-trained model and tokenizer.\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39mis_initialized():\n\u001B[0;32m--> 103\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistributed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_process_group\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnccl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_parallel_is_initialized():\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m model_parallel_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:79\u001B[0m, in \u001B[0;36m_exception_logger.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _T:\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 79\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[1;32m     81\u001B[0m         msg_dict \u001B[38;5;241m=\u001B[39m _get_msg_dict(func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:93\u001B[0m, in \u001B[0;36m_time_logger.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _T:\n\u001B[1;32m     92\u001B[0m     t1 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime_ns()\n\u001B[0;32m---> 93\u001B[0m     func_return \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime_ns() \u001B[38;5;241m-\u001B[39m t1\n\u001B[1;32m     96\u001B[0m     msg_dict \u001B[38;5;241m=\u001B[39m _get_msg_dict(func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:1361\u001B[0m, in \u001B[0;36minit_process_group\u001B[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001B[0m\n\u001B[1;32m   1357\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m store \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1358\u001B[0m     rendezvous_iterator \u001B[38;5;241m=\u001B[39m rendezvous(\n\u001B[1;32m   1359\u001B[0m         not_none(init_method), rank, world_size, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m   1360\u001B[0m     )\n\u001B[0;32m-> 1361\u001B[0m     store, rank, world_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mrendezvous_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1362\u001B[0m     store\u001B[38;5;241m.\u001B[39mset_timeout(timeout)\n\u001B[1;32m   1364\u001B[0m     \u001B[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001B[39;00m\n\u001B[1;32m   1365\u001B[0m     \u001B[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/distributed/rendezvous.py:246\u001B[0m, in \u001B[0;36m_env_rendezvous_handler\u001B[0;34m(url, timeout, **kwargs)\u001B[0m\n\u001B[1;32m    244\u001B[0m     rank \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(query_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrank\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 246\u001B[0m     rank \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[43m_get_env_or_raise\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRANK\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworld_size\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m query_dict:\n\u001B[1;32m    249\u001B[0m     world_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(query_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworld_size\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/distributed/rendezvous.py:231\u001B[0m, in \u001B[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001B[0;34m(env_var)\u001B[0m\n\u001B[1;32m    229\u001B[0m env_val \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(env_var, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m env_val:\n\u001B[0;32m--> 231\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _env_error(env_var)\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_val\n",
      "\u001B[0;31mValueError\u001B[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import fire\n",
    "from termcolor import cprint\n",
    "import torch\n",
    "\n",
    "from models.llama3.reference_impl.generation import Llama\n",
    "\n",
    "# Задаем параметры\n",
    "max_batch_size: int = 32\n",
    "max_seq_len: int = 2048\n",
    "model_parallel_size = 1  # Убедитесь, что используется одна GPU/одна машина\n",
    "max_gen_len = 50\n",
    "\n",
    "# Указываем путь к токенизатору\n",
    "tokenizer_path = str(\"/tmp/checkpoints/Llama3.2-3B-Instruct/tokenizer.model\")\n",
    "\n",
    "# Инициализируем генератор текста\n",
    "generator = Llama.build(\n",
    "    ckpt_dir='/tmp/checkpoints/Llama3.2-3B-Instruct/',\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    "    model_parallel_size=model_parallel_size,\n",
    ")\n",
    "\n",
    "# Примеры запросов для генерации текста\n",
    "prompts = [\n",
    "    \"The color of the sky is blue but sometimes it can also be\",\n",
    "    \"\"\"\\\n",
    "apple is pomme,\n",
    "bannana is banane,\n",
    "cherry is\"\"\",\n",
    "    \"1, 2, 3, 5, 8, 13\",\n",
    "    \"ba ba black sheep, have you any wool?\",\n",
    "]\n",
    "\n",
    "# Обрабатываем каждый запрос\n",
    "for prompt in prompts:\n",
    "    result = generator.text_completion(\n",
    "        prompt,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        max_gen_len=max_gen_len,\n",
    "        logprobs=False,\n",
    "    )\n",
    "\n",
    "    cprint(f\"{prompt}\", end=\"\")\n",
    "    cprint(f\"{result.generation}\", color=\"yellow\")\n",
    "    print(\"\\n==================================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export RANK=0\n",
    "!export WORLD_SIZE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /tmp/checkpoints/Llama3.2-3B-Instruct/.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m LlamaTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_path, local_files_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Load the model\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mLlamaForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Test the model with a sample prompt\u001B[39;00m\n\u001B[1;32m     16\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat is the capital of France?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:3558\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3553\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m   3554\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError no file named \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001B[38;5;250m \u001B[39mvariant)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m found in directory\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3555\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3556\u001B[0m         )\n\u001B[1;32m   3557\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3558\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m   3559\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError no file named \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_add_variant(WEIGHTS_NAME,\u001B[38;5;250m \u001B[39mvariant)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001B[38;5;250m \u001B[39mvariant)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3560\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTF2_WEIGHTS_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTF_WEIGHTS_NAME\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.index\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m or \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mFLAX_WEIGHTS_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m found in directory\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3561\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3562\u001B[0m         )\n\u001B[1;32m   3563\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001B[1;32m   3564\u001B[0m     archive_file \u001B[38;5;241m=\u001B[39m pretrained_model_name_or_path\n",
      "\u001B[0;31mOSError\u001B[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /tmp/checkpoints/Llama3.2-3B-Instruct/."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Specify paths to your files\n",
    "model_path = \"/tmp/checkpoints/Llama3.2-3B-Instruct/\"\n",
    "model_checkpoint = model_path + \"consolidated.00.pth\"\n",
    "tokenizer_path = model_path + \"tokenizer.model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Load the model\n",
    "model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Test the model with a sample prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(inputs.input_ids, max_length=50)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in /tmp/checkpoints/Llama3.2-3B-Instruct/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[64], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Укажи путь к модели\u001B[39;00m\n\u001B[1;32m      5\u001B[0m local_model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/checkpoints/Llama3.2-3B-Instruct/\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Замени на фактический путь к твоей скачанной модели\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m \u001B[43mtransformers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_model_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch_dtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbfloat16\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Убедись, что GPU доступен\u001B[39;49;00m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Пример генерации текста\u001B[39;00m\n\u001B[1;32m     15\u001B[0m output \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello, I\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm trying to\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py:805\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[1;32m    802\u001B[0m                 adapter_config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m    803\u001B[0m                 model \u001B[38;5;241m=\u001B[39m adapter_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase_model_name_or_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 805\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[43mAutoConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    806\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_from_pipeline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\n\u001B[1;32m    807\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    808\u001B[0m     hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39m_commit_hash\n\u001B[1;32m    810\u001B[0m custom_tasks \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1038\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m   1035\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(pretrained_model_name_or_path):\n\u001B[1;32m   1036\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m CONFIG_MAPPING[pattern]\u001B[38;5;241m.\u001B[39mfrom_dict(config_dict, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munused_kwargs)\n\u001B[0;32m-> 1038\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized model in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1040\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould have a `model_type` key in its \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, or contain one of the following strings \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1041\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min its name: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(CONFIG_MAPPING\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1042\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: Unrecognized model in /tmp/checkpoints/Llama3.2-3B-Instruct/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zoedepth"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Укажи путь к модели\n",
    "local_model_path = \"/tmp/checkpoints/Llama3.2-3B-Instruct/\"  # Замени на фактический путь к твоей скачанной модели\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=local_model_path,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",  # Убедись, что GPU доступен\n",
    ")\n",
    "\n",
    "# Пример генерации текста\n",
    "output = pipeline(\"Hello, I'm trying to\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-66f7dd71-0866daef0992513439496324;5356795e-4389-4960-a876-33c353611dc6)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 406\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mGatedRepoError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 403\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001B[0m, in \u001B[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    100\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1232\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1232\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1233\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[1;32m   1234\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1235\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[1;32m   1236\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1238\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1240\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[1;32m   1241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1245\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1246\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[1;32m   1247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1249\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1339\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[0m\n\u001B[1;32m   1338\u001B[0m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[0;32m-> 1339\u001B[0m     \u001B[43m_raise_on_head_call_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead_call_error\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1341\u001B[0m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1854\u001B[0m, in \u001B[0;36m_raise_on_head_call_error\u001B[0;34m(head_call_error, force_download, local_files_only)\u001B[0m\n\u001B[1;32m   1852\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, RepositoryNotFoundError) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, GatedRepoError):\n\u001B[1;32m   1853\u001B[0m     \u001B[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001B[39;00m\n\u001B[0;32m-> 1854\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[1;32m   1855\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1856\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1746\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1746\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1747\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\n\u001B[1;32m   1748\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1666\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[1;32m   1665\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[0;32m-> 1666\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHEAD\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1670\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1671\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1673\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1674\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1675\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:364\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[0;32m--> 364\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    371\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:388\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    387\u001B[0m response \u001B[38;5;241m=\u001B[39m get_session()\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m--> 388\u001B[0m \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    389\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:423\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    420\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    421\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Client Error.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot access gated repo for url \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    422\u001B[0m     )\n\u001B[0;32m--> 423\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _format(GatedRepoError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m error_message \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccess to this resource is disabled.\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mGatedRepoError\u001B[0m: 401 Client Error. (Request ID: Root=1-66f7dd71-0866daef0992513439496324;5356795e-4389-4960-a876-33c353611dc6)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[63], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m \u001B[43mtransformers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m  \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m  \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch_dtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbfloat16\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m  \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py:805\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[1;32m    802\u001B[0m                 adapter_config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m    803\u001B[0m                 model \u001B[38;5;241m=\u001B[39m adapter_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase_model_name_or_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 805\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[43mAutoConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    806\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_from_pipeline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\n\u001B[1;32m    807\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    808\u001B[0m     hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39m_commit_hash\n\u001B[1;32m    810\u001B[0m custom_tasks \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1006\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m   1003\u001B[0m trust_remote_code \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrust_remote_code\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m   1004\u001B[0m code_revision \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_revision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1006\u001B[0m config_dict, unused_kwargs \u001B[38;5;241m=\u001B[39m \u001B[43mPretrainedConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1007\u001B[0m has_remote_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1008\u001B[0m has_local_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m CONFIG_MAPPING\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py:567\u001B[0m, in \u001B[0;36mPretrainedConfig.get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    565\u001B[0m original_kwargs \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)\n\u001B[1;32m    566\u001B[0m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[0;32m--> 567\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}, kwargs\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py:626\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    622\u001B[0m configuration_file \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_configuration_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, CONFIG_NAME) \u001B[38;5;28;01mif\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m gguf_file\n\u001B[1;32m    624\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    625\u001B[0m     \u001B[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001B[39;00m\n\u001B[0;32m--> 626\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfiguration_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    630\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    638\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    639\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    641\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, kwargs\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:421\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    419\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_gated_repo:\n\u001B[1;32m    420\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m resolved_file\n\u001B[0;32m--> 421\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    422\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to have access to it at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    423\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    424\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    426\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    427\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a local folder and is not a valid model identifier \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    428\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlisted on \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf this is a private repository, make sure to pass a token \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    429\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    430\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`token=<your_token>`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    431\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mOSError\u001B[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-66f7dd71-0866daef0992513439496324;5356795e-4389-4960-a876-33c353611dc6)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "  \"text-generation\",\n",
    "  model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "  model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "  device=\"cuda\",\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model /tmp/checkpoints/Llama3.2-3B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3558, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /tmp/checkpoints/Llama3.2-3B-Instruct.\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3558, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /tmp/checkpoints/Llama3.2-3B-Instruct.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[72], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[1;32m      4\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/checkpoints/Llama3.2-3B-Instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m pipe \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbfloat16\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# device_map=\"cuda\"\u001B[39;49;00m\n\u001B[1;32m     11\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m pipe(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe key to life is\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py:896\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[1;32m    894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m framework \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    895\u001B[0m     model_classes \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m: targeted_task[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m: targeted_task[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m]}\n\u001B[0;32m--> 896\u001B[0m     framework, model \u001B[38;5;241m=\u001B[39m \u001B[43minfer_framework_load_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    897\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    898\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_classes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    899\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    900\u001B[0m \u001B[43m        \u001B[49m\u001B[43mframework\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mframework\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    901\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    903\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    904\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    906\u001B[0m model_config \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\n\u001B[1;32m    907\u001B[0m hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_commit_hash\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py:301\u001B[0m, in \u001B[0;36minfer_framework_load_model\u001B[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001B[0m\n\u001B[1;32m    299\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m class_name, trace \u001B[38;5;129;01min\u001B[39;00m all_traceback\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    300\u001B[0m             error \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhile loading with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclass_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, an error is thrown:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mtrace\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 301\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    302\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not load model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with any of the following classes: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclass_tuple\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. See the original errors:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00merror\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    303\u001B[0m         )\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m framework \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    306\u001B[0m     framework \u001B[38;5;241m=\u001B[39m infer_framework(model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Could not load model /tmp/checkpoints/Llama3.2-3B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3558, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /tmp/checkpoints/Llama3.2-3B-Instruct.\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3558, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /tmp/checkpoints/Llama3.2-3B-Instruct.\n\n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"/tmp/checkpoints/Llama3.2-3B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    # device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "pipe(\"The key to life is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.11/site-packages (0.34.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.26.0) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2023.7.22)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'R2-Tuning'...\n",
      "remote: Enumerating objects: 137, done.\u001B[K\n",
      "remote: Counting objects: 100% (137/137), done.\u001B[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001B[K\n",
      "remote: Total 137 (delta 63), reused 116 (delta 46), pack-reused 0 (from 0)\u001B[K\n",
      "Receiving objects: 100% (137/137), 602.58 KiB | 3.40 MiB/s, done.\n",
      "Resolving deltas: 100% (63/63), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yeliudev/R2-Tuning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workdir/R2-Tuning\n",
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Collecting git+https://github.com/openai/CLIP.git@a1d0717 (from -r requirements.txt (line 1))\n",
      "  Cloning https://github.com/openai/CLIP.git (to revision a1d0717) to /tmp/pip-req-build-7n2rytt2\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-7n2rytt2\n",
      "\u001B[33m  WARNING: Did not find branch or tag 'a1d0717', assuming revision or ref.\u001B[0m\u001B[33m\n",
      "\u001B[0m  Running command git checkout -q a1d0717\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d0717\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting decord==0.6.0 (from -r requirements.txt (line 2))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/decord/0.6.0/decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.6/13.6 MB\u001B[0m \u001B[31m141.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting ffmpeg-python==0.2 (from -r requirements.txt (line 3))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/ffmpeg-python/0.2.0/ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting nncore==0.4.4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nncore/0.4.4/nncore-0.4.4.tar.gz (79 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.6/79.6 kB\u001B[0m \u001B[31m11.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting scikit-learn==1.4.1.post1 (from -r requirements.txt (line 5))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/scikit-learn/1.4.1.post1/scikit_learn-1.4.1.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.1/12.1 MB\u001B[0m \u001B[31m141.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting torch==2.2.1 (from -r requirements.txt (line 6))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/torch/2.2.1/torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m755.6/755.6 MB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting torchvision==0.17.1 (from -r requirements.txt (line 7))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/torchvision/0.17.1/torchvision-0.17.1-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.9/6.9 MB\u001B[0m \u001B[31m129.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from decord==0.6.0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting future (from ffmpeg-python==0.2->-r requirements.txt (line 3))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/future/1.0.0/future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m491.3/491.3 kB\u001B[0m \u001B[31m98.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting h5py>=3.10 (from nncore==0.4.4->-r requirements.txt (line 4))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/h5py/3.12.1/h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m110.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting joblib>=1.3 (from nncore==0.4.4->-r requirements.txt (line 4))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/joblib/1.4.2/joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m301.8/301.8 kB\u001B[0m \u001B[31m24.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jsonlines>=4 (from nncore==0.4.4->-r requirements.txt (line 4))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/jsonlines/4.0.0/jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: pyyaml>=6 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tabulate>=0.9 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r requirements.txt (line 4)) (0.9.0)\n",
      "Collecting tensorboard>=2.16 (from nncore==0.4.4->-r requirements.txt (line 4))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/tensorboard/2.18.0/tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m120.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: termcolor>=2.4 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r requirements.txt (line 4)) (2.4.0)\n",
      "Collecting opencv-python>=4.9 (from nncore==0.4.4->-r requirements.txt (line 4))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/opencv-python/4.10.0.84/opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.5/62.5 MB\u001B[0m \u001B[31m71.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting scipy>=1.6.0 (from scikit-learn==1.4.1.post1->-r requirements.txt (line 5))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/scipy/1.14.1/scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.2/41.2 MB\u001B[0m \u001B[31m89.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn==1.4.1.post1->-r requirements.txt (line 5))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/threadpoolctl/3.5.0/threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r requirements.txt (line 6)) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r requirements.txt (line 6)) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r requirements.txt (line 6)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r requirements.txt (line 6)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r requirements.txt (line 6)) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r requirements.txt (line 6)) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->-r requirements.txt (line 6))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cudnn-cu12/8.9.2.26/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001B[2K     \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.8/731.7 MB\u001B[0m \u001B[31m86.1 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m^C\n",
      "\u001B[2K     \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.8/731.7 MB\u001B[0m \u001B[31m86.1 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\n",
      "\u001B[?25h\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Collecting nncore==0.4.4\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nncore/0.4.4/nncore-0.4.4.tar.gz (79 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting h5py>=3.10 (from nncore==0.4.4)\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/h5py/3.12.1/h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "Collecting joblib>=1.3 (from nncore==0.4.4)\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/joblib/1.4.2/joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting jsonlines>=4 (from nncore==0.4.4)\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/jsonlines/4.0.0/jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=6 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (2.31.0)\n",
      "Requirement already satisfied: tabulate>=0.9 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (0.9.0)\n",
      "Collecting tensorboard>=2.16 (from nncore==0.4.4)\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/tensorboard/2.18.0/tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: termcolor>=2.4 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (2.4.0)\n",
      "Collecting opencv-python>=4.9 (from nncore==0.4.4)\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/opencv-python/4.10.0.84/opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonlines>=4->nncore==0.4.4) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4) (2023.7.22)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=2.16->nncore==0.4.4)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/absl-py/2.1.0/absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.7/133.7 kB\u001B[0m \u001B[31m7.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (1.66.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=2.16->nncore==0.4.4)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/markdown/3.7/Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m106.3/106.3 kB\u001B[0m \u001B[31m14.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (4.25.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.16->nncore==0.4.4)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/tensorboard-data-server/0.7.2/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/6.6 MB\u001B[0m \u001B[31m69.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting werkzeug>=1.0.1 (from tensorboard>=2.16->nncore==0.4.4)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/werkzeug/3.0.4/werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m227.6/227.6 kB\u001B[0m \u001B[31m15.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.16->nncore==0.4.4) (2.1.3)\n",
      "Building wheels for collected packages: nncore\n",
      "  Building wheel for nncore (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for nncore: filename=nncore-0.4.4-py3-none-any.whl size=107904 sha256=1f208a873691df685d651ab993723d9ff2f463820104a9914a5fcd67ef0eb51a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/45/31/73/f1c34778e42b3ee68a5bd633380a1267eec06540e44debcdb6\n",
      "Successfully built nncore\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, opencv-python, markdown, jsonlines, joblib, h5py, absl-py, tensorboard, nncore\n",
      "Successfully installed absl-py-2.1.0 h5py-3.12.1 joblib-1.4.2 jsonlines-4.0.0 markdown-3.7 nncore-0.4.4 opencv-python-4.10.0.84 tensorboard-2.18.0 tensorboard-data-server-0.7.2 werkzeug-3.0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "%cd R2-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3c870fcc822206e9f0882fb70744ed14.mp4\n",
      "audio_0a1bd95990029433d45dd0a7b15d9b50.mp4\n",
      "audio_0c6e4cdaa192d1ae58b99bc9f35891b9.mp4\n",
      "audio_0cee2037e28ed045c425a6dbeca1bff2.mp4\n",
      "audio_0cf954675163c4fe8f1313b6e6bb8c19.mp4\n",
      "audio_0d0cb2e712923b3bace3320beec9476e.mp4\n",
      "audio_0d1a9fa41857d58c005ccde4ca517685.mp4\n",
      "audio_1c78f72bd8c56515486ad6a1eb464d7d.mp4\n",
      "audio_1c8af4543c345fd937bdc234a72712f1.mp4\n",
      "audio_1d0f431c41252a7fac46f8f499b7bebc.mp4\n",
      "audio_2a97b86fce59b2bc28c5ee907e600cc9.mp4\n",
      "audio_2c32ef7b31d84f080c86e7278cb510f6.mp4\n",
      "audio_2c5bdce3e9e2c8b9db713d9f2c196820.mp4\n",
      "audio_2c745419906a934efc4201da29f31c3b.mp4\n",
      "audio_2caa27c4926768a14395b856a7cfa580.mp4\n",
      "audio_2e4eca888ca4bb3fc09f967fcc500eb2.mp4\n",
      "audio_2e57018a4e5282ee4378f4ae7d412682.mp4\n",
      "audio_3c562522e2fff930245e02270570209e.mp4\n",
      "audio_3c870fcc822206e9f0882fb70744ed14.mp4\n",
      "audio_3d09581a8dc3029466120aefbf610315.mp4\n",
      "audio_3d1ea59037afbbe33d7c7b64f1e4f7c2.mp4\n",
      "audio_3ebd6174dfbe38b4ad6b20722b8361fe.mp4\n",
      "audio_4b401117aa62d1d8a32fde35b34a4f64.mp4\n",
      "audio_4c82ef411fc1e7ab07321605c4917b58.mp4\n",
      "audio_4c866aa742af11c4cf04867167c80e40.mp4\n",
      "audio_4ca4cd2fa4ec98ba74c7b78bf753d127.mp4\n",
      "audio_4cb8085a4154b91c5e6288c33b70949c.mp4\n",
      "audio_4ccda842416c3349791afadb3efb9078.mp4\n",
      "checkpoints\n",
      "go-init.sock\n",
      "pip-ephem-wheel-cache-1o_el6_c\n",
      "pip-install-xf0yx6yp\n",
      "pip-pip-egg-info-8oibock5\n",
      "pip-pip-egg-info-j22lifsl\n",
      "pip-req-build-7n2rytt2\n",
      "pip-unpack-0uk616if\n",
      "pip-unpack-6aiucy2h\n",
      "pip-unpack-7pas51k0\n",
      "pip-unpack-vzhnwnua\n",
      "pip-unpack-ztmdnbyi\n",
      "test.txt\n",
      "tmpg3au0ppy\n",
      "tmp.podPBemcco\n",
      "tmp.wQIYubSY3r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd R2-Tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs   LICENSE  README.md\t     setup.cfg\n",
      "datasets  models   requirements.txt  tools\n"
     ]
    }
   ],
   "source": [
    "!ls \n",
    "//R2-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Collecting git+https://github.com/openai/CLIP.git@a1d0717 (from -r /workdir/R2-Tuning/requirements.txt (line 1))\n",
      "  Cloning https://github.com/openai/CLIP.git (to revision a1d0717) to /tmp/pip-req-build-zjb5_dwy\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-zjb5_dwy\n",
      "\u001B[33m  WARNING: Did not find branch or tag 'a1d0717', assuming revision or ref.\u001B[0m\u001B[33m\n",
      "\u001B[0m  Running command git checkout -q a1d0717\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d0717\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting decord==0.6.0 (from -r /workdir/R2-Tuning/requirements.txt (line 2))\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/decord/0.6.0/decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
      "Collecting ffmpeg-python==0.2 (from -r /workdir/R2-Tuning/requirements.txt (line 3))\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/ffmpeg-python/0.2.0/ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: nncore==0.4.4 in /opt/conda/lib/python3.11/site-packages (from -r /workdir/R2-Tuning/requirements.txt (line 4)) (0.4.4)\n",
      "Collecting scikit-learn==1.4.1.post1 (from -r /workdir/R2-Tuning/requirements.txt (line 5))\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/scikit-learn/1.4.1.post1/scikit_learn-1.4.1.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Collecting torch==2.2.1 (from -r /workdir/R2-Tuning/requirements.txt (line 6))\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/torch/2.2.1/torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
      "Collecting torchvision==0.17.1 (from -r /workdir/R2-Tuning/requirements.txt (line 7))\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/torchvision/0.17.1/torchvision-0.17.1-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from decord==0.6.0->-r /workdir/R2-Tuning/requirements.txt (line 2)) (1.26.4)\n",
      "Collecting future (from ffmpeg-python==0.2->-r /workdir/R2-Tuning/requirements.txt (line 3))\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/future/1.0.0/future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Requirement already satisfied: h5py>=3.10 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.12.1)\n",
      "Requirement already satisfied: joblib>=1.3 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: jsonlines>=4 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: pyyaml>=6 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tabulate>=0.9 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: tensorboard>=2.16 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: termcolor>=2.4 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: opencv-python>=4.9 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (4.10.0.84)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn==1.4.1.post1->-r /workdir/R2-Tuning/requirements.txt (line 5))\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/scipy/1.14.1/scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.4.1.post1->-r /workdir/R2-Tuning/requirements.txt (line 5))\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/threadpoolctl/3.5.0/threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cudnn-cu12/8.9.2.26/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-nccl-cu12/2.19.3/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m28.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.105)\n",
      "Collecting triton==2.2.0 (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/triton/2.2.0/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m167.9/167.9 MB\u001B[0m \u001B[31m27.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision==0.17.1->-r /workdir/R2-Tuning/requirements.txt (line 7)) (10.4.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.6.68)\n",
      "Collecting ftfy (from clip==1.0->-r /workdir/R2-Tuning/requirements.txt (line 1))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/ftfy/6.2.3/ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.0/43.0 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.11/site-packages (from clip==1.0->-r /workdir/R2-Tuning/requirements.txt (line 1)) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from clip==1.0->-r /workdir/R2-Tuning/requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonlines>=4->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2023.7.22)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (1.66.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (4.25.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.0.4)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->clip==1.0->-r /workdir/R2-Tuning/requirements.txt (line 1))\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/wcwidth/0.2.13/wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=44a4c19c6245b10586c655dfb85f80f28b613d579c431f01f2bf74b253cf5c75\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9iplnyzp/wheels/fc/76/81/ed2f6be5c303471743b9634aec2f3e56162dddec0eac9bc7bd\n",
      "Successfully built clip\n",
      "Installing collected packages: wcwidth, triton, threadpoolctl, scipy, nvidia-nccl-cu12, nvidia-cudnn-cu12, future, ftfy, decord, scikit-learn, ffmpeg-python, torch, torchvision, clip\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.8\n",
      "    Uninstalling wcwidth-0.2.8:\n",
      "      Successfully uninstalled wcwidth-0.2.8\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1\n",
      "    Uninstalling torch-2.4.1:\n",
      "      Successfully uninstalled torch-2.4.1\n",
      "Successfully installed clip-1.0 decord-0.6.0 ffmpeg-python-0.2.0 ftfy-6.2.3 future-1.0.0 nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 scikit-learn-1.4.1.post1 scipy-1.14.1 threadpoolctl-3.5.0 torch-2.2.1 torchvision-0.17.1 triton-2.2.0 wcwidth-0.2.13\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /workdir/R2-Tuning/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: nncore==0.4.4 in /opt/conda/lib/python3.11/site-packages (0.4.4)\n",
      "Requirement already satisfied: h5py>=3.10 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (3.12.1)\n",
      "Requirement already satisfied: joblib>=1.3 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (1.4.2)\n",
      "Requirement already satisfied: jsonlines>=4 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.26 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=6 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (2.31.0)\n",
      "Requirement already satisfied: tabulate>=0.9 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (0.9.0)\n",
      "Requirement already satisfied: tensorboard>=2.16 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (2.18.0)\n",
      "Requirement already satisfied: termcolor>=2.4 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (2.4.0)\n",
      "Requirement already satisfied: opencv-python>=4.9 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4) (4.10.0.84)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonlines>=4->nncore==0.4.4) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4) (2023.7.22)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (1.66.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (3.7)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (4.25.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.16->nncore==0.4.4) (2.1.3)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nncore==0.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/pip/24.2/pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m76.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-24.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Collecting git+https://github.com/openai/CLIP.git@a1d0717 (from -r /workdir/R2-Tuning/requirements.txt (line 1))\n",
      "  Cloning https://github.com/openai/CLIP.git (to revision a1d0717) to /tmp/pip-req-build-vehixh3g\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-vehixh3g\n",
      "\u001B[33m  WARNING: Did not find branch or tag 'a1d0717', assuming revision or ref.\u001B[0m\u001B[33m\n",
      "\u001B[0m  Running command git checkout -q a1d0717\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d0717\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: decord==0.6.0 in /opt/conda/lib/python3.11/site-packages (from -r /workdir/R2-Tuning/requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: ffmpeg-python==0.2 in /opt/conda/lib/python3.11/site-packages (from -r /workdir/R2-Tuning/requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: nncore==0.4.4 in /opt/conda/lib/python3.11/site-packages (from -r /workdir/R2-Tuning/requirements.txt (line 4)) (0.4.4)\n",
      "Requirement already satisfied: scikit-learn==1.4.1.post1 in /opt/conda/lib/python3.11/site-packages (from -r /workdir/R2-Tuning/requirements.txt (line 5)) (1.4.1.post1)\n",
      "Requirement already satisfied: torch==2.2.1 in /opt/conda/lib/python3.11/site-packages (from -r /workdir/R2-Tuning/requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: torchvision==0.17.1 in /opt/conda/lib/python3.11/site-packages (from -r /workdir/R2-Tuning/requirements.txt (line 7)) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from decord==0.6.0->-r /workdir/R2-Tuning/requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.11/site-packages (from ffmpeg-python==0.2->-r /workdir/R2-Tuning/requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: h5py>=3.10 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.12.1)\n",
      "Requirement already satisfied: joblib>=1.3 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: jsonlines>=4 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: pyyaml>=6 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tabulate>=0.9 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: tensorboard>=2.16 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: termcolor>=2.4 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: opencv-python>=4.9 in /opt/conda/lib/python3.11/site-packages (from nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (4.10.0.84)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn==1.4.1.post1->-r /workdir/R2-Tuning/requirements.txt (line 5)) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn==1.4.1.post1->-r /workdir/R2-Tuning/requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision==0.17.1->-r /workdir/R2-Tuning/requirements.txt (line 7)) (10.4.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (12.6.68)\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.11/site-packages (from clip==1.0->-r /workdir/R2-Tuning/requirements.txt (line 1)) (6.2.3)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.11/site-packages (from clip==1.0->-r /workdir/R2-Tuning/requirements.txt (line 1)) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from clip==1.0->-r /workdir/R2-Tuning/requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonlines>=4->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2023.7.22)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (1.66.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (4.25.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard>=2.16->nncore==0.4.4->-r /workdir/R2-Tuning/requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.11/site-packages (from ftfy->clip==1.0->-r /workdir/R2-Tuning/requirements.txt (line 1)) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch==2.2.1->-r /workdir/R2-Tuning/requirements.txt (line 6)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# !ls /workdir/R2-Tuning/\n",
    "!pip install -r /workdir/R2-Tuning/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.16.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.22.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/nncore/__init__.py:11: UserWarning: Please install opencv-python to enable 'nncore.image' and 'nncore.video'\n",
      "  warn(\"Please install opencv-python to enable 'nncore.image' and \"\n",
      "Building model from /workdir/R2-Tuning/configs/qvhighlights/r2_tuning_qvhighlights.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/workdir/R2-Tuning/tools/inference.py\", line 85, in <module>\n",
      "    main()\n",
      "  File \"/workdir/R2-Tuning/tools/inference.py\", line 59, in main\n",
      "    model = build_model(cfg.model, dist=False).eval()\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'eval'\n"
     ]
    }
   ],
   "source": [
    "!python \"/workdir/R2-Tuning/tools/inference.py\" '/tmp/3c870fcc822206e9f0882fb70744ed14.mp4' 'viral clip' --config \"/workdir/R2-Tuning/configs/qvhighlights/r2_tuning_qvhighlights.py\" --checkpoint \"/workdir/R2-Tuning/r2_tuning_qvhighlights-ed516355.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mcfg\u001B[49m\u001B[38;5;241m.\u001B[39mmodel)\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m build_model(cfg\u001B[38;5;241m.\u001B[39mmodel, dist\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(model)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "print(cfg.model)\n",
    "model = build_model(cfg.model, dist=False)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/pip/24.2/pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m71.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-24.2\n",
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.11/site-packages (0.34.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.26.0) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2023.7.22)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f5c153a58842fbbbc2885d4e17069f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n",
      "CPU times: user 1.78 s, sys: 597 ms, total: 2.38 s\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Path to the model files\n",
    "model_dir = \"/tmp/llama-3.2-transformers-3b-instruct-v1\"\n",
    "\n",
    "# Load the model from the local path\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    # torch_dtype=torch.float32,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    "    local_files_only=True  # Ensure the model is loaded from local files\n",
    ")\n",
    "\n",
    "# Load the tokenizer from the local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 561 ms, sys: 20.1 ms, total: 581 ms\n",
      "Wall time: 580 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_dir,\n",
    "    local_files_only=True  # Ensure processor is loaded from local files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|startoftext|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are an expert in identifying viral content from video transcripts. I will provide a conversation fragment, and your task is to detect the most engaging 10-20 second time range that has the highest potential to go viral. Your analysis should focus on the following key viral content elements:\n",
      "\n",
      "1. **Emotional Trigger Points**: Moments that provoke strong emotions such as humor, excitement, or inspiration.\n",
      "   - Example: \"I just won a million dollars!\" or \"This changed my life forever.\"\n",
      "\n",
      "2. **Relatable or Shared Experiences**: Identify sections that reflect common experiences or challenges, making them easily relatable.\n",
      "   - Example: \"We've all been through tough times, but here's how I overcame mine.\"\n",
      "\n",
      "3. **Surprising or Unexpected Moments**: Highlight sections with surprising, shocking, or unexpected elements that grab attention.\n",
      "   - Example: \"And then the cake exploded all over the kitchen!\"\n",
      "\n",
      "4. **Memorable or Catchy Phrases**: Catchy phrases or repetitive language that can turn into memes or be easily shared.\n",
      "   - Example: \"Keep calm and carry on\" or \"Can't stop, won't stop.\"\n",
      "\n",
      "5. **Call to Action**: Identify moments where the video encourages viewers to take action, such as sharing, commenting, or participating in a challenge.\n",
      "   - Example: \"Share this with a friend who needs to hear it.\"\n",
      "\n",
      "6. **Humor and Entertainment**: Look for funny moments, punchlines, or quirky expressions that make the video more shareable.\n",
      "   - Example: \"I thought it was water, but it was vinegar—big mistake!\"\n",
      "\n",
      "**Instructions**:\n",
      "- Analyze the provided transcript, identify the most likely viral moments, and explain why you selected them based on the viral content elements above.\n",
      "- Your response should be in Russian.\n",
      "  \n",
      "Here is the transcript:\n",
      "\n",
      "\"\n",
      "[00:00.000 --> 00:03.000]  [музыка]\n",
      "[00:30.000 --> 00:49.080]  Ямайка.\n",
      "[00:49.080 --> 00:52.020]  Как много песен про этот маленький остров в Карибском\n",
      "[00:52.020 --> 00:53.020]  бассейне.\n",
      "[00:53.020 --> 00:55.320]  Мамочки, какой вид!\n",
      "[00:55.320 --> 00:58.720]  Туристы едут сюда за теплым морем, музыкой регги.\n",
      "[00:58.720 --> 01:00.220]  Слушай, а может что-нибудь поешь?\n",
      "[01:00.220 --> 01:03.720]  Шумными вечеринками и удивительной культурой с ее людьми.\n",
      "[01:05.720 --> 01:07.720]  Люди Ямайки - это душа.\n",
      "[01:07.720 --> 01:10.720]  Теплая, отзывчивая и свободная.\n",
      "[01:11.220 --> 01:12.720]  Можно нас обнять.\n",
      "[01:12.720 --> 01:16.720]  Прикоснуться к ней - лучшее, что можно сделать в этой стране.\n",
      "[01:23.720 --> 01:24.720]  Yeah, we speak, man.\n",
      "[01:28.720 --> 01:30.720]  Знаете ли вы что-то о Ямайке?\n",
      "[01:30.720 --> 01:32.200]  Прекрасная страна.\n",
      "[01:32.200 --> 01:33.200]  А что там?\n",
      "[01:33.200 --> 01:34.840]  Там много ростоманов.\n",
      "[01:34.840 --> 01:40.200]  Ну они всегда, знаете, покуривают там травку, отдыхают где-то.\n",
      "[01:40.200 --> 01:40.960]  Ого!\n",
      "[01:40.960 --> 01:41.720]  Да.\n",
      "[01:41.720 --> 01:43.480]  Это я в опасное место попаду.\n",
      "[01:43.480 --> 01:44.360]  Не надо бояться.\n",
      "[01:44.360 --> 01:45.720]  Почему? А зачем?\n",
      "[01:45.720 --> 01:48.920]  Не знаю, мне сразу в голову приходит изображение Боб Марли.\n",
      "[01:48.920 --> 01:50.040]  Дред и все дела.\n",
      "[01:50.040 --> 01:51.720]  А знаешь, что там музыка классная?\n",
      "[01:51.720 --> 01:52.720]  Рэги.\n",
      "[01:52.720 --> 01:54.320]  Все, что там покуривают.\n",
      "[01:54.320 --> 01:55.720]  Очень бедная страна.\n",
      "[01:55.720 --> 01:57.920]  Ямайка.\n",
      "[01:57.920 --> 01:59.920]  Я слышал, что там вроде скучно сейчас.\n",
      "[01:59.920 --> 02:01.920]  Ну, эммм...\n",
      "[02:01.920 --> 02:04.920]  Есть места более интересные, чем Ямайка.\n",
      "[02:04.920 --> 02:07.920]  Вам не легче найти какую-то другую страну более близкую?\n",
      "[02:07.920 --> 02:09.920]  На самом деле, я думаю, что страна не самая богатая, поэтому...\n",
      "[02:09.920 --> 02:12.920]  Ямайка, вот это вот всё чил.\n",
      "[02:12.920 --> 02:15.920]  Пляж, кокосы.\n",
      "[02:15.920 --> 02:17.920]  Ну, фрукты.\n",
      "[02:17.920 --> 02:20.920]  Поэтому я думаю, вы будете не разочарованы.\n",
      "[02:20.920 --> 02:22.920]  Как вы считаете, много денег я там оставлю?\n",
      "[02:22.920 --> 02:24.920]  Думаю, что да.\n",
      "[02:24.920 --> 02:27.920]  Мы не знаем чего про Ямайку, может там не были.\n",
      "[02:27.920 --> 02:30.160]  Думаю, что оно того стоит в любом случае.\n",
      "[02:30.160 --> 02:55.160]  [музыка]\n",
      "[02:55.160 --> 02:58.400]  Всем привет! Я прилетел на Ямайку!\n",
      "[02:58.400 --> 03:01.400]  Спрашивали вот вы, зачем же так далеко ехать?\n",
      "[03:01.400 --> 03:05.400]  Ну конечно, если останавливаться в отеле всё включено, понять сложно.\n",
      "[03:05.400 --> 03:08.400]  А ведь главное, для чего стоит посетить это место,\n",
      "[03:08.400 --> 03:11.900]  это люди, культура и образ жизни.\n",
      "[03:11.900 --> 03:14.160]  Ну и чтобы развенчать все стереотипы,\n",
      "[03:14.160 --> 03:16.160]  нужно выходить за предел отеля.\n",
      "[03:16.160 --> 03:19.160]  Ногами, вот так. Оп, и пошёл.\n",
      "[03:25.160 --> 03:31.160]  Некоторые люди считают, что страна это не богатая, а ямайцы все нищие.\n",
      "[03:31.160 --> 03:36.160]  Ну-ну, эту нищету отчетливо видно на улице города.\n",
      "[03:36.160 --> 03:41.160]  Если ямайцы о чем-то и парятся, так это о том, как они выглядят.\n",
      "[03:41.160 --> 03:46.160]  На первом месте у них дорогие шмотки, украшения или последняя модель телефона.\n",
      "[03:46.160 --> 03:50.160]  Короче, все, чем можно похвастаться за пределами квартиры.\n",
      "[03:50.160 --> 03:56.160]  Да, тут нет небоскребов, но ямайцам они и не нужны.\n",
      "[03:56.160 --> 03:58.960]  Тут вечное лето и теплое Карибское море.\n",
      "[03:58.960 --> 04:06.280]  Зачем на райском острове небоскребы?\n",
      "[04:06.280 --> 04:09.060]  Все ассоциируют Ямайку лишь с Бобом Марли, но здесь\n",
      "[04:09.060 --> 04:12.000]  есть и другие известные герои на весь мир.\n",
      "[04:12.000 --> 04:16.920]  Ямайка - это родина самых быстрых людей в мире.\n",
      "[04:16.920 --> 04:19.640]  Усейн Болт родом с Ямайки и в забегах на короткие\n",
      "[04:19.640 --> 04:22.780]  дистанции, он восемь раз побивал мировой рекорд.\n",
      "[04:22.780 --> 04:25.180]  Национальный герой современных дней.\n",
      "[04:25.180 --> 04:27.860]  Он даже открыл свой бар для поклонников.\n",
      "[04:27.860 --> 04:29.780]  Кстати, как вы думаете, из чего сделана фигура у\n",
      "[04:29.780 --> 04:31.700]  Сейна Болта?\n",
      "[04:31.700 --> 04:32.700]  Из болтов, конечно.\n",
      "[04:32.700 --> 04:45.660]  Туристы, приезжающие в Монтега-Вей, сразу спешат на знаменитый\n",
      "[04:45.660 --> 04:50.800]  на весь мир пляж Dr's Cave и платит за вход целых 8 баксов.\n",
      "[04:50.800 --> 04:53.560]  На самом же деле в Монтеге есть бесплатный общественный\n",
      "[04:53.560 --> 04:56.520]  пляж, открывшийся всего пару лет назад, о котором\n",
      "[04:56.520 --> 04:58.400]  многие туристы не знают.\n",
      "[04:58.400 --> 05:00.160]  Здесь собираются местные.\n",
      "[05:00.160 --> 05:10.160]  Тут и парк с беговыми дорожками классный, и красивые ямайки,\n",
      "[05:10.160 --> 05:11.440]  и Карибское море.\n",
      "[05:11.440 --> 05:34.400]  Можно купаться бесплатно!\n",
      "[05:34.400 --> 05:37.520]  Вместо отеля я решил поселиться в естхаусе, ну во-первых,\n",
      "[05:37.520 --> 05:40.760]  чтобы сэкономить, а во-вторых, здесь очень прикольная локация,\n",
      "[05:40.760 --> 05:43.360]  самый центр Монтега Бэй и пляж рядом.\n",
      "[05:51.160 --> 05:52.960]  Ну ничего, уютненько очень даже.\n",
      "[05:52.960 --> 05:54.960]  Прикольный номер, спасибо вам большое.\n",
      "[05:54.960 --> 05:57.560]  Слушай, ты такой приятный парень.\n",
      "[05:57.560 --> 05:59.960]  Думаю, я смогу найти для тебя номер получше.\n",
      "[05:59.960 --> 06:01.360]  Реально? Для меня?\n",
      "[06:01.360 --> 06:02.560]  Да, да, конечно.\n",
      "[06:02.560 --> 06:03.960]  А почему именно мне?\n",
      "[06:03.960 --> 06:06.160]  Ты довольно милый и шляпа прикольная,\n",
      "[06:06.160 --> 06:09.360]  так что в качестве комплимента давай в номер классом выше.\n",
      "[06:09.360 --> 06:11.960]  - Спасибо большое, очень приятно. - Только за шляпу.\n",
      "[06:11.960 --> 06:13.360]  Спасибо огромное.\n",
      "[06:13.360 --> 06:18.040]  Тебе нужно пройти в соседние здания, думаю, тебе там понравится гораздо больше.\n",
      "[06:18.040 --> 06:19.040]  Спасибо, пойдем.\n",
      "[06:22.360 --> 06:26.960]  Просто что происходит, мне вообще в шоке, мне дали номер лучше.\n",
      "[06:26.960 --> 06:29.640]  За те же деньги вообще никакой доплаты не взяли.\n",
      "[06:30.640 --> 06:33.000]  Вот это гостеприимство, ребята, вот это мне нравится.\n",
      "[06:33.000 --> 06:35.520]  И причем сказали, что такое часто с туристами делают.\n",
      "[06:39.360 --> 06:41.800]  Добро пожаловать в мой отель. Я Карл.\n",
      "[06:41.800 --> 06:43.560]  Я Саша из России.\n",
      "[06:43.560 --> 06:46.200]  Приятно познакомиться. Рад тебя видеть.\n",
      "[06:46.200 --> 06:48.760]  Ты являешься владельцем трех отелей?\n",
      "[06:49.920 --> 06:52.320]  Просто глядя на тебя, складывается ощущение,\n",
      "[06:52.320 --> 06:54.320]  что ты обычный парень ямайский.\n",
      "[06:54.320 --> 06:56.520]  Ну а как еще я должен выглядеть?\n",
      "[06:56.520 --> 06:58.160]  Я такой, какой есть.\n",
      "[06:58.160 --> 07:00.080]  Прости, я просто не могу не заметить,\n",
      "[07:00.080 --> 07:02.720]  но ты реально очень похож на Боба Марли.\n",
      "[07:02.720 --> 07:05.480]  Ну, Боб Марли - это же мой дядя.\n",
      "[07:05.480 --> 07:06.680]  Серьезно?\n",
      "[07:06.680 --> 07:07.640]  Смотри.\n",
      "[07:07.640 --> 07:09.640]  Это сейчас правда, ты не шутишь?\n",
      "[07:09.640 --> 07:11.640]  Ну да, по крови.\n",
      "[07:11.640 --> 07:15.640]  Вау! Я стою рядом с родственником Боба Марли.\n",
      "[07:15.640 --> 07:17.640]  Можно я просто пожму тебе руку?\n",
      "[07:17.640 --> 07:19.640]  Это невероятно круто!\n",
      "[07:19.640 --> 07:23.640]  Где еще можно встретить человека, который является прямым родственником Боба Марли?\n",
      "[07:23.640 --> 07:29.640]  Я не верю своим глазам до сих пор, что я на Ямайке, а то, что я встретил живого родственника Боба Марли, с ума сойти вообще.\n",
      "[07:29.640 --> 07:31.640]  Надеюсь, что тебе у нас понравится.\n",
      "[07:31.640 --> 07:33.640]  Спасибо огромное!\n",
      "[07:37.640 --> 07:56.240]  Ну тут ребята вообще по-царски. Как мне тут нравится. Готов?\n",
      "[07:56.240 --> 07:57.240]  А-а-а!\n",
      "[07:59.600 --> 08:00.880]  Всё, остаюсь.\n",
      "[08:04.320 --> 08:07.360]  Так, это что такое? Это такой вид из окна?\n",
      "[08:08.080 --> 08:09.760]  Мамочки, какой вид!\n",
      "[08:11.320 --> 08:14.040]  На тот самый легендарный доктор Кейв.\n",
      "[08:14.280 --> 08:16.520]  Он просто у меня как на ладони отсюда.\n",
      "[08:16.840 --> 08:18.840]  Тут вот вечером приведу.\n",
      "[08:19.520 --> 08:22.280]  Ямаек, они мне расскажут, как живут здесь.\n",
      "[08:22.720 --> 08:23.720]  Как хорошо.\n",
      "[08:24.480 --> 08:25.480]  Вот.\n",
      "[08:25.480 --> 08:30.920]  Вот это место для меня! Всё! Жизнь удалась!\n",
      "[08:47.720 --> 08:51.200]  Кстати, я тебе по секрету скажу, это тачка Карла,\n",
      "[08:51.200 --> 08:56.960]  владельца на минуточку трех отелей на Ямайке. Можешь себе представить? Ямайцы вообще не парятся.\n",
      "[08:56.960 --> 09:04.880]  Давай, сфоткай меня. Это все-таки машина племянника Боб Марли. Давай снимочек на память.\n",
      "[09:04.880 --> 09:12.600]  Конечно, все считают Ямайку островом ростоманов, и многие туристы готовы просто любого человека с\n",
      "[09:12.600 --> 09:18.200]  дредами назвать ростоманом, но на самом деле тема достаточно сложная, и знакомьтесь, Виктория,\n",
      "[09:18.200 --> 09:22.200]  человек, который согласился мне рассказать, помочь разобраться с этим вопросом.\n",
      "[09:22.200 --> 09:26.700]  Скажи мне, пожалуйста, Ямайка это действительно остров людей с дредами и ростоманов?\n",
      "[09:26.700 --> 09:32.400]  Ростоманы действительно начались на Ямайке, но Ямайка не остров ростоманов.\n",
      "[09:32.400 --> 09:40.200]  Официально по статистике на Ямайке живет всего 10% ростоманов, а все остальные люди, как я.\n",
      "[09:40.200 --> 09:46.200]  Или вообще без дредов. А кто-то это делает по простой причине. Представь себе, что ты пришел на пляж.\n",
      "[09:46.200 --> 09:48.800]  Стоит две лавки сувенирные.\n",
      "[09:48.800 --> 09:51.200]  В одной лысый чувак торгует, а в другой — Ростоман.\n",
      "[09:51.200 --> 09:52.600]  Ты в какой купишь сувениры?\n",
      "[09:52.600 --> 09:54.200]  Я к Ростоману пойду.\n",
      "[09:54.200 --> 09:55.600]  Само собой, да?\n",
      "[09:55.600 --> 09:56.400]  Я же на Ямайке.\n",
      "[09:56.400 --> 10:01.400]  А я прислала себе, что ты девушка, которая приехала на Ямайку в поисках развлечений.\n",
      "[10:01.400 --> 10:04.200]  Тут лысый чувак, а тут с дредами. Ты кого выберешь?\n",
      "[10:04.200 --> 10:08.200]  Это мне уже сложнее. Это вы уже решаете, девушки, кого вы выберете.\n",
      "[10:08.200 --> 10:11.800]  Собственно, поэтому появилось название «Rent a Dread», да?\n",
      "[10:11.800 --> 10:14.800]  То есть чуваки, которые делают дреды в коммерческих целях,\n",
      "[10:14.800 --> 10:17.120]  чтобы привлечь к себе больше внимания.\n",
      "[10:17.120 --> 10:20.280]  Но в основном у нас идет обязательное условие.\n",
      "[10:20.280 --> 10:24.080]  Женщинам с дредами можно, потому что это стиль, а\n",
      "[10:24.080 --> 10:27.040]  мужчины должны сбрить голосы на лосах.\n",
      "[10:27.040 --> 10:31.560]  Иначе их на работу в приличные места не возьмут из-за дред.\n",
      "[10:31.560 --> 10:35.720]  Растоман на Ямайке для большинства людей - это\n",
      "[10:35.720 --> 10:38.600]  ленивый человек, который ничего не делает, не хочет\n",
      "[10:38.600 --> 10:41.640]  работать, идет против системы и курит травку.\n",
      "[10:41.640 --> 10:44.760]  И, например, богатые обеспеченные семьи, если узнают, что\n",
      "[10:44.760 --> 10:50.680]  их дочь влюбилась в ростомана, могут ее даже дома запереть, не выпускать из дома, чтобы она не начала\n",
      "[10:50.680 --> 10:58.560]  с этим встречаться и не опозорила семью. Поэтому ростоманов на Ямайке не любят. Это удивительно,\n",
      "[10:58.560 --> 11:04.280]  конечно. Вот с первым стереотипом мы покончили. Теперь скажи мне, травку здесь все курят? На самом\n",
      "[11:04.280 --> 11:10.640]  деле на Ямайке курит всего 7% населения. Для сравнения, в Америке 25%. Помимо прочего,\n",
      "[11:10.640 --> 11:16.480]  я тебя сейчас удивлю, не все, кто курят траву ростоманы, и не все, кто ростоманы курят траву.\n",
      "[11:16.480 --> 11:22.320]  Мало того, некоторые ростоманы могут быть лысыми и курить траву, а некоторые ростоманы могут быть\n",
      "[11:22.320 --> 11:28.840]  дредами, но при этом не курить траву. Но почему-то благодаря, видимо, Бобу Марли, да, у общественности\n",
      "[11:28.840 --> 11:34.960]  сложилось мнение, что на Имайке все курят, но чаще всего основная масса тех, кто курит, это...\n",
      "[11:34.960 --> 11:39.960]  Да, некрасиво так говорить, но я скажу, это низшие слои населения.\n",
      "[11:39.960 --> 11:45.960]  И самое забавное, что вообще-то курить в общественных местах у нас запрещено.\n",
      "[11:45.960 --> 11:48.960]  И штраф за это 500 баксов. Американских.\n",
      "[11:48.960 --> 11:51.960]  А вы думали, здесь курят, где попало на пропалую?\n",
      "[11:51.960 --> 11:58.960]  Ты еще сказала про вот этих коммерческих ростоманов, которые с дредами ходят в целях побольше продать.\n",
      "[11:58.960 --> 12:00.960]  А настоящих-то где искать?\n",
      "[12:00.960 --> 12:06.560]  Ну настоящие на самом деле не очень спешат контактировать с туристами.\n",
      "[12:06.560 --> 12:09.960]  Но если хочешь, пошли покажу. Пойдем. Пойдем.\n",
      "[12:30.960 --> 12:49.960]  Итак, растаманы живут очень по-разному.\n",
      "[12:49.960 --> 12:54.160]  Есть целые поселения растаманов в глухих джунглях, в горах.\n",
      "[12:54.160 --> 12:57.000]  Есть растаманы-одиночки, которые куда-нибудь удрали\n",
      "[12:57.000 --> 12:58.000]  и живут.\n",
      "[12:58.000 --> 13:00.840]  Но большинство расто живет среди обычных людей.\n",
      "[13:00.840 --> 13:09.040]  Это деревня, куда мы сейчас приехали, знаменита тем, что здесь живет огромнейшее количество настоящих некоммерческих ростоманов.\n",
      "[13:09.040 --> 13:10.240]  Сейчас познакомлю.\n",
      "[13:13.320 --> 13:14.120]  Робби!\n",
      "[13:14.120 --> 13:16.120]  - You! - Hey!\n",
      "[13:16.120 --> 13:20.520]  - Hello, Vicky! - Hey, Vicky!\n",
      "[13:20.520 --> 13:23.120]  - How are you? - I'm okay.\n",
      "[13:23.120 --> 13:26.240]  - Так давно не виделись. - I'm glad to see you.\n",
      "[13:26.240 --> 13:28.960]  - Ты в порядке? - I bring my friend.\n",
      "[13:28.960 --> 13:30.360]  - Okay. - Hello.\n",
      "\"\n",
      "\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Я нашел несколько моментов, которые могут стать viral-ом:\n",
      "\n",
      "1.  Вопрос: \"Как вы думаете, из чего сделана фигура у Сейна Болта?\" с ответом: \"Из болтов, конечно.\" - Это funny и интересный вопрос, который может привлечь внимание и вызвать смех.\n",
      "2.  Вопрос: \"Знаете ли вы, что там много ростоманов?\" с ответом: \"Ну они всегда, знаете, покуривают там травку, отдыхают где-то.\" - Этот ответ funny и интересен, поскольку он показывает, что ростоманы не только имеют дреды, но и курят травку.\n",
      "3.  Вопрос: \"Там много ростоманов?\" с ответом: \"Ну они всегда, знаете, покуривают там травку, отдыхают где-то.\" - Этот ответ funny и интересен, поскольку он показывает, что ростоманы не только имеют дреды, но и курят травку.\n",
      "4.  Вопрос: \"Кстати, я тебе по секрету скажу, это тачка Карла, владельца на минуточку трех отелей на Ямайке. Можешь себе представить? Ямайцы вообще не парятся.\" - Этот вопрос интересен, поскольку он показывает, что владелец отелей на Ямайке имеет тачку, что может быть интересным фактом для туристов.\n",
      "5.  Вопрос: \"Ты еще сказала про вот этих коммерческих ростоманов, которые с дредами ходят в целях побольше продать.\" - Этот вопрос funny и интересен, поскольку он показывает, что коммерческие ростоманы не являются настоящими ростоманами.\n",
      "6.  Вопрос: \"Ну настоящие на самом деле не очень спешат контактировать с туристами.\" - Этот вопрос funny и интересен, поскольку он показывает, что ростоманы не являютсяvery friendly people.\n",
      "7.  Вопрос: \"Но если хочешь, пошли покажу. Пойдем. Пойдем.\" - этот вопрос funny и интересен, поскольку он показывает, что владелец отелей на Ямайке готов показать туристам настоящих ростоманов.\n",
      "8.  Вопрос: \"Итак, растаманы живут очень по-разному.\" - этот вопрос funny и интересен, поскольку он показывает, что ростоманы не являются одной и той же люди, как о них говорят в сtereotype.\n",
      "9.  Вопрос: \"Есть растаманы-одиночки, которые куда-нибудь удрали и живут.\" - этот вопрос funny и интересен, поскольку он показывает, что ростоманы не являются только мужчинами.\n",
      "10. Вопрос: \"Но большинство расто живет среди обычных людей.\" - этот вопрос funny и интересен, поскольку он показывает, что ростоманы не являются изгнанными людьми.<|eot_id|>\n",
      "CPU times: user 18.5 s, sys: 75.2 ms, total: 18.5 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time  \n",
    "# Prompt Setup with Best Practices for LLaMA Model\n",
    "prompt = \"\"\"\n",
    "<|startoftext|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "You are an expert in identifying viral content from video transcripts. I will provide a conversation fragment, and your task is to detect the most engaging 10-20 second time range that has the highest potential to go viral. Your analysis should focus on the following key viral content elements:\n",
    "\n",
    "1. **Emotional Trigger Points**: Moments that provoke strong emotions such as humor, excitement, or inspiration.\n",
    "   - Example: \"I just won a million dollars!\" or \"This changed my life forever.\"\n",
    "\n",
    "2. **Relatable or Shared Experiences**: Identify sections that reflect common experiences or challenges, making them easily relatable.\n",
    "   - Example: \"We've all been through tough times, but here's how I overcame mine.\"\n",
    "\n",
    "3. **Surprising or Unexpected Moments**: Highlight sections with surprising, shocking, or unexpected elements that grab attention.\n",
    "   - Example: \"And then the cake exploded all over the kitchen!\"\n",
    "\n",
    "4. **Memorable or Catchy Phrases**: Catchy phrases or repetitive language that can turn into memes or be easily shared.\n",
    "   - Example: \"Keep calm and carry on\" or \"Can't stop, won't stop.\"\n",
    "\n",
    "5. **Call to Action**: Identify moments where the video encourages viewers to take action, such as sharing, commenting, or participating in a challenge.\n",
    "   - Example: \"Share this with a friend who needs to hear it.\"\n",
    "\n",
    "6. **Humor and Entertainment**: Look for funny moments, punchlines, or quirky expressions that make the video more shareable.\n",
    "   - Example: \"I thought it was water, but it was vinegar—big mistake!\"\n",
    "\n",
    "**Instructions**:\n",
    "- Analyze the provided transcript, identify the most likely viral moments, and explain why you selected them based on the viral content elements above.\n",
    "- Your response should be in Russian.\n",
    "  \n",
    "Here is the transcript:\n",
    "\n",
    "\"\n",
    "[00:00.000 --> 00:03.000]  [музыка]\n",
    "[00:30.000 --> 00:49.080]  Ямайка.\n",
    "[00:49.080 --> 00:52.020]  Как много песен про этот маленький остров в Карибском\n",
    "[00:52.020 --> 00:53.020]  бассейне.\n",
    "[00:53.020 --> 00:55.320]  Мамочки, какой вид!\n",
    "[00:55.320 --> 00:58.720]  Туристы едут сюда за теплым морем, музыкой регги.\n",
    "[00:58.720 --> 01:00.220]  Слушай, а может что-нибудь поешь?\n",
    "[01:00.220 --> 01:03.720]  Шумными вечеринками и удивительной культурой с ее людьми.\n",
    "[01:05.720 --> 01:07.720]  Люди Ямайки - это душа.\n",
    "[01:07.720 --> 01:10.720]  Теплая, отзывчивая и свободная.\n",
    "[01:11.220 --> 01:12.720]  Можно нас обнять.\n",
    "[01:12.720 --> 01:16.720]  Прикоснуться к ней - лучшее, что можно сделать в этой стране.\n",
    "[01:23.720 --> 01:24.720]  Yeah, we speak, man.\n",
    "[01:28.720 --> 01:30.720]  Знаете ли вы что-то о Ямайке?\n",
    "[01:30.720 --> 01:32.200]  Прекрасная страна.\n",
    "[01:32.200 --> 01:33.200]  А что там?\n",
    "[01:33.200 --> 01:34.840]  Там много ростоманов.\n",
    "[01:34.840 --> 01:40.200]  Ну они всегда, знаете, покуривают там травку, отдыхают где-то.\n",
    "[01:40.200 --> 01:40.960]  Ого!\n",
    "[01:40.960 --> 01:41.720]  Да.\n",
    "[01:41.720 --> 01:43.480]  Это я в опасное место попаду.\n",
    "[01:43.480 --> 01:44.360]  Не надо бояться.\n",
    "[01:44.360 --> 01:45.720]  Почему? А зачем?\n",
    "[01:45.720 --> 01:48.920]  Не знаю, мне сразу в голову приходит изображение Боб Марли.\n",
    "[01:48.920 --> 01:50.040]  Дред и все дела.\n",
    "[01:50.040 --> 01:51.720]  А знаешь, что там музыка классная?\n",
    "[01:51.720 --> 01:52.720]  Рэги.\n",
    "[01:52.720 --> 01:54.320]  Все, что там покуривают.\n",
    "[01:54.320 --> 01:55.720]  Очень бедная страна.\n",
    "[01:55.720 --> 01:57.920]  Ямайка.\n",
    "[01:57.920 --> 01:59.920]  Я слышал, что там вроде скучно сейчас.\n",
    "[01:59.920 --> 02:01.920]  Ну, эммм...\n",
    "[02:01.920 --> 02:04.920]  Есть места более интересные, чем Ямайка.\n",
    "[02:04.920 --> 02:07.920]  Вам не легче найти какую-то другую страну более близкую?\n",
    "[02:07.920 --> 02:09.920]  На самом деле, я думаю, что страна не самая богатая, поэтому...\n",
    "[02:09.920 --> 02:12.920]  Ямайка, вот это вот всё чил.\n",
    "[02:12.920 --> 02:15.920]  Пляж, кокосы.\n",
    "[02:15.920 --> 02:17.920]  Ну, фрукты.\n",
    "[02:17.920 --> 02:20.920]  Поэтому я думаю, вы будете не разочарованы.\n",
    "[02:20.920 --> 02:22.920]  Как вы считаете, много денег я там оставлю?\n",
    "[02:22.920 --> 02:24.920]  Думаю, что да.\n",
    "[02:24.920 --> 02:27.920]  Мы не знаем чего про Ямайку, может там не были.\n",
    "[02:27.920 --> 02:30.160]  Думаю, что оно того стоит в любом случае.\n",
    "[02:30.160 --> 02:55.160]  [музыка]\n",
    "[02:55.160 --> 02:58.400]  Всем привет! Я прилетел на Ямайку!\n",
    "[02:58.400 --> 03:01.400]  Спрашивали вот вы, зачем же так далеко ехать?\n",
    "[03:01.400 --> 03:05.400]  Ну конечно, если останавливаться в отеле всё включено, понять сложно.\n",
    "[03:05.400 --> 03:08.400]  А ведь главное, для чего стоит посетить это место,\n",
    "[03:08.400 --> 03:11.900]  это люди, культура и образ жизни.\n",
    "[03:11.900 --> 03:14.160]  Ну и чтобы развенчать все стереотипы,\n",
    "[03:14.160 --> 03:16.160]  нужно выходить за предел отеля.\n",
    "[03:16.160 --> 03:19.160]  Ногами, вот так. Оп, и пошёл.\n",
    "[03:25.160 --> 03:31.160]  Некоторые люди считают, что страна это не богатая, а ямайцы все нищие.\n",
    "[03:31.160 --> 03:36.160]  Ну-ну, эту нищету отчетливо видно на улице города.\n",
    "[03:36.160 --> 03:41.160]  Если ямайцы о чем-то и парятся, так это о том, как они выглядят.\n",
    "[03:41.160 --> 03:46.160]  На первом месте у них дорогие шмотки, украшения или последняя модель телефона.\n",
    "[03:46.160 --> 03:50.160]  Короче, все, чем можно похвастаться за пределами квартиры.\n",
    "[03:50.160 --> 03:56.160]  Да, тут нет небоскребов, но ямайцам они и не нужны.\n",
    "[03:56.160 --> 03:58.960]  Тут вечное лето и теплое Карибское море.\n",
    "[03:58.960 --> 04:06.280]  Зачем на райском острове небоскребы?\n",
    "[04:06.280 --> 04:09.060]  Все ассоциируют Ямайку лишь с Бобом Марли, но здесь\n",
    "[04:09.060 --> 04:12.000]  есть и другие известные герои на весь мир.\n",
    "[04:12.000 --> 04:16.920]  Ямайка - это родина самых быстрых людей в мире.\n",
    "[04:16.920 --> 04:19.640]  Усейн Болт родом с Ямайки и в забегах на короткие\n",
    "[04:19.640 --> 04:22.780]  дистанции, он восемь раз побивал мировой рекорд.\n",
    "[04:22.780 --> 04:25.180]  Национальный герой современных дней.\n",
    "[04:25.180 --> 04:27.860]  Он даже открыл свой бар для поклонников.\n",
    "[04:27.860 --> 04:29.780]  Кстати, как вы думаете, из чего сделана фигура у\n",
    "[04:29.780 --> 04:31.700]  Сейна Болта?\n",
    "[04:31.700 --> 04:32.700]  Из болтов, конечно.\n",
    "[04:32.700 --> 04:45.660]  Туристы, приезжающие в Монтега-Вей, сразу спешат на знаменитый\n",
    "[04:45.660 --> 04:50.800]  на весь мир пляж Dr's Cave и платит за вход целых 8 баксов.\n",
    "[04:50.800 --> 04:53.560]  На самом же деле в Монтеге есть бесплатный общественный\n",
    "[04:53.560 --> 04:56.520]  пляж, открывшийся всего пару лет назад, о котором\n",
    "[04:56.520 --> 04:58.400]  многие туристы не знают.\n",
    "[04:58.400 --> 05:00.160]  Здесь собираются местные.\n",
    "[05:00.160 --> 05:10.160]  Тут и парк с беговыми дорожками классный, и красивые ямайки,\n",
    "[05:10.160 --> 05:11.440]  и Карибское море.\n",
    "[05:11.440 --> 05:34.400]  Можно купаться бесплатно!\n",
    "[05:34.400 --> 05:37.520]  Вместо отеля я решил поселиться в естхаусе, ну во-первых,\n",
    "[05:37.520 --> 05:40.760]  чтобы сэкономить, а во-вторых, здесь очень прикольная локация,\n",
    "[05:40.760 --> 05:43.360]  самый центр Монтега Бэй и пляж рядом.\n",
    "[05:51.160 --> 05:52.960]  Ну ничего, уютненько очень даже.\n",
    "[05:52.960 --> 05:54.960]  Прикольный номер, спасибо вам большое.\n",
    "[05:54.960 --> 05:57.560]  Слушай, ты такой приятный парень.\n",
    "[05:57.560 --> 05:59.960]  Думаю, я смогу найти для тебя номер получше.\n",
    "[05:59.960 --> 06:01.360]  Реально? Для меня?\n",
    "[06:01.360 --> 06:02.560]  Да, да, конечно.\n",
    "[06:02.560 --> 06:03.960]  А почему именно мне?\n",
    "[06:03.960 --> 06:06.160]  Ты довольно милый и шляпа прикольная,\n",
    "[06:06.160 --> 06:09.360]  так что в качестве комплимента давай в номер классом выше.\n",
    "[06:09.360 --> 06:11.960]  - Спасибо большое, очень приятно. - Только за шляпу.\n",
    "[06:11.960 --> 06:13.360]  Спасибо огромное.\n",
    "[06:13.360 --> 06:18.040]  Тебе нужно пройти в соседние здания, думаю, тебе там понравится гораздо больше.\n",
    "[06:18.040 --> 06:19.040]  Спасибо, пойдем.\n",
    "[06:22.360 --> 06:26.960]  Просто что происходит, мне вообще в шоке, мне дали номер лучше.\n",
    "[06:26.960 --> 06:29.640]  За те же деньги вообще никакой доплаты не взяли.\n",
    "[06:30.640 --> 06:33.000]  Вот это гостеприимство, ребята, вот это мне нравится.\n",
    "[06:33.000 --> 06:35.520]  И причем сказали, что такое часто с туристами делают.\n",
    "[06:39.360 --> 06:41.800]  Добро пожаловать в мой отель. Я Карл.\n",
    "[06:41.800 --> 06:43.560]  Я Саша из России.\n",
    "[06:43.560 --> 06:46.200]  Приятно познакомиться. Рад тебя видеть.\n",
    "[06:46.200 --> 06:48.760]  Ты являешься владельцем трех отелей?\n",
    "[06:49.920 --> 06:52.320]  Просто глядя на тебя, складывается ощущение,\n",
    "[06:52.320 --> 06:54.320]  что ты обычный парень ямайский.\n",
    "[06:54.320 --> 06:56.520]  Ну а как еще я должен выглядеть?\n",
    "[06:56.520 --> 06:58.160]  Я такой, какой есть.\n",
    "[06:58.160 --> 07:00.080]  Прости, я просто не могу не заметить,\n",
    "[07:00.080 --> 07:02.720]  но ты реально очень похож на Боба Марли.\n",
    "[07:02.720 --> 07:05.480]  Ну, Боб Марли - это же мой дядя.\n",
    "[07:05.480 --> 07:06.680]  Серьезно?\n",
    "[07:06.680 --> 07:07.640]  Смотри.\n",
    "[07:07.640 --> 07:09.640]  Это сейчас правда, ты не шутишь?\n",
    "[07:09.640 --> 07:11.640]  Ну да, по крови.\n",
    "[07:11.640 --> 07:15.640]  Вау! Я стою рядом с родственником Боба Марли.\n",
    "[07:15.640 --> 07:17.640]  Можно я просто пожму тебе руку?\n",
    "[07:17.640 --> 07:19.640]  Это невероятно круто!\n",
    "[07:19.640 --> 07:23.640]  Где еще можно встретить человека, который является прямым родственником Боба Марли?\n",
    "[07:23.640 --> 07:29.640]  Я не верю своим глазам до сих пор, что я на Ямайке, а то, что я встретил живого родственника Боба Марли, с ума сойти вообще.\n",
    "[07:29.640 --> 07:31.640]  Надеюсь, что тебе у нас понравится.\n",
    "[07:31.640 --> 07:33.640]  Спасибо огромное!\n",
    "[07:37.640 --> 07:56.240]  Ну тут ребята вообще по-царски. Как мне тут нравится. Готов?\n",
    "[07:56.240 --> 07:57.240]  А-а-а!\n",
    "[07:59.600 --> 08:00.880]  Всё, остаюсь.\n",
    "[08:04.320 --> 08:07.360]  Так, это что такое? Это такой вид из окна?\n",
    "[08:08.080 --> 08:09.760]  Мамочки, какой вид!\n",
    "[08:11.320 --> 08:14.040]  На тот самый легендарный доктор Кейв.\n",
    "[08:14.280 --> 08:16.520]  Он просто у меня как на ладони отсюда.\n",
    "[08:16.840 --> 08:18.840]  Тут вот вечером приведу.\n",
    "[08:19.520 --> 08:22.280]  Ямаек, они мне расскажут, как живут здесь.\n",
    "[08:22.720 --> 08:23.720]  Как хорошо.\n",
    "[08:24.480 --> 08:25.480]  Вот.\n",
    "[08:25.480 --> 08:30.920]  Вот это место для меня! Всё! Жизнь удалась!\n",
    "[08:47.720 --> 08:51.200]  Кстати, я тебе по секрету скажу, это тачка Карла,\n",
    "[08:51.200 --> 08:56.960]  владельца на минуточку трех отелей на Ямайке. Можешь себе представить? Ямайцы вообще не парятся.\n",
    "[08:56.960 --> 09:04.880]  Давай, сфоткай меня. Это все-таки машина племянника Боб Марли. Давай снимочек на память.\n",
    "[09:04.880 --> 09:12.600]  Конечно, все считают Ямайку островом ростоманов, и многие туристы готовы просто любого человека с\n",
    "[09:12.600 --> 09:18.200]  дредами назвать ростоманом, но на самом деле тема достаточно сложная, и знакомьтесь, Виктория,\n",
    "[09:18.200 --> 09:22.200]  человек, который согласился мне рассказать, помочь разобраться с этим вопросом.\n",
    "[09:22.200 --> 09:26.700]  Скажи мне, пожалуйста, Ямайка это действительно остров людей с дредами и ростоманов?\n",
    "[09:26.700 --> 09:32.400]  Ростоманы действительно начались на Ямайке, но Ямайка не остров ростоманов.\n",
    "[09:32.400 --> 09:40.200]  Официально по статистике на Ямайке живет всего 10% ростоманов, а все остальные люди, как я.\n",
    "[09:40.200 --> 09:46.200]  Или вообще без дредов. А кто-то это делает по простой причине. Представь себе, что ты пришел на пляж.\n",
    "[09:46.200 --> 09:48.800]  Стоит две лавки сувенирные.\n",
    "[09:48.800 --> 09:51.200]  В одной лысый чувак торгует, а в другой — Ростоман.\n",
    "[09:51.200 --> 09:52.600]  Ты в какой купишь сувениры?\n",
    "[09:52.600 --> 09:54.200]  Я к Ростоману пойду.\n",
    "[09:54.200 --> 09:55.600]  Само собой, да?\n",
    "[09:55.600 --> 09:56.400]  Я же на Ямайке.\n",
    "[09:56.400 --> 10:01.400]  А я прислала себе, что ты девушка, которая приехала на Ямайку в поисках развлечений.\n",
    "[10:01.400 --> 10:04.200]  Тут лысый чувак, а тут с дредами. Ты кого выберешь?\n",
    "[10:04.200 --> 10:08.200]  Это мне уже сложнее. Это вы уже решаете, девушки, кого вы выберете.\n",
    "[10:08.200 --> 10:11.800]  Собственно, поэтому появилось название «Rent a Dread», да?\n",
    "[10:11.800 --> 10:14.800]  То есть чуваки, которые делают дреды в коммерческих целях,\n",
    "[10:14.800 --> 10:17.120]  чтобы привлечь к себе больше внимания.\n",
    "[10:17.120 --> 10:20.280]  Но в основном у нас идет обязательное условие.\n",
    "[10:20.280 --> 10:24.080]  Женщинам с дредами можно, потому что это стиль, а\n",
    "[10:24.080 --> 10:27.040]  мужчины должны сбрить голосы на лосах.\n",
    "[10:27.040 --> 10:31.560]  Иначе их на работу в приличные места не возьмут из-за дред.\n",
    "[10:31.560 --> 10:35.720]  Растоман на Ямайке для большинства людей - это\n",
    "[10:35.720 --> 10:38.600]  ленивый человек, который ничего не делает, не хочет\n",
    "[10:38.600 --> 10:41.640]  работать, идет против системы и курит травку.\n",
    "[10:41.640 --> 10:44.760]  И, например, богатые обеспеченные семьи, если узнают, что\n",
    "[10:44.760 --> 10:50.680]  их дочь влюбилась в ростомана, могут ее даже дома запереть, не выпускать из дома, чтобы она не начала\n",
    "[10:50.680 --> 10:58.560]  с этим встречаться и не опозорила семью. Поэтому ростоманов на Ямайке не любят. Это удивительно,\n",
    "[10:58.560 --> 11:04.280]  конечно. Вот с первым стереотипом мы покончили. Теперь скажи мне, травку здесь все курят? На самом\n",
    "[11:04.280 --> 11:10.640]  деле на Ямайке курит всего 7% населения. Для сравнения, в Америке 25%. Помимо прочего,\n",
    "[11:10.640 --> 11:16.480]  я тебя сейчас удивлю, не все, кто курят траву ростоманы, и не все, кто ростоманы курят траву.\n",
    "[11:16.480 --> 11:22.320]  Мало того, некоторые ростоманы могут быть лысыми и курить траву, а некоторые ростоманы могут быть\n",
    "[11:22.320 --> 11:28.840]  дредами, но при этом не курить траву. Но почему-то благодаря, видимо, Бобу Марли, да, у общественности\n",
    "[11:28.840 --> 11:34.960]  сложилось мнение, что на Имайке все курят, но чаще всего основная масса тех, кто курит, это...\n",
    "[11:34.960 --> 11:39.960]  Да, некрасиво так говорить, но я скажу, это низшие слои населения.\n",
    "[11:39.960 --> 11:45.960]  И самое забавное, что вообще-то курить в общественных местах у нас запрещено.\n",
    "[11:45.960 --> 11:48.960]  И штраф за это 500 баксов. Американских.\n",
    "[11:48.960 --> 11:51.960]  А вы думали, здесь курят, где попало на пропалую?\n",
    "[11:51.960 --> 11:58.960]  Ты еще сказала про вот этих коммерческих ростоманов, которые с дредами ходят в целях побольше продать.\n",
    "[11:58.960 --> 12:00.960]  А настоящих-то где искать?\n",
    "[12:00.960 --> 12:06.560]  Ну настоящие на самом деле не очень спешат контактировать с туристами.\n",
    "[12:06.560 --> 12:09.960]  Но если хочешь, пошли покажу. Пойдем. Пойдем.\n",
    "[12:30.960 --> 12:49.960]  Итак, растаманы живут очень по-разному.\n",
    "[12:49.960 --> 12:54.160]  Есть целые поселения растаманов в глухих джунглях, в горах.\n",
    "[12:54.160 --> 12:57.000]  Есть растаманы-одиночки, которые куда-нибудь удрали\n",
    "[12:57.000 --> 12:58.000]  и живут.\n",
    "[12:58.000 --> 13:00.840]  Но большинство расто живет среди обычных людей.\n",
    "[13:00.840 --> 13:09.040]  Это деревня, куда мы сейчас приехали, знаменита тем, что здесь живет огромнейшее количество настоящих некоммерческих ростоманов.\n",
    "[13:09.040 --> 13:10.240]  Сейчас познакомлю.\n",
    "[13:13.320 --> 13:14.120]  Робби!\n",
    "[13:14.120 --> 13:16.120]  - You! - Hey!\n",
    "[13:16.120 --> 13:20.520]  - Hello, Vicky! - Hey, Vicky!\n",
    "[13:20.520 --> 13:23.120]  - How are you? - I'm okay.\n",
    "[13:23.120 --> 13:26.240]  - Так давно не виделись. - I'm glad to see you.\n",
    "[13:26.240 --> 13:28.960]  - Ты в порядке? - I bring my friend.\n",
    "[13:28.960 --> 13:30.360]  - Okay. - Hello.\n",
    "\"\n",
    "\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# 프롬프트를 모델에 입력 가능한 형식(tensor)으로 변환\n",
    "inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 텍스트 생성\n",
    "output = model.generate(**inputs, max_new_tokens=3000)\n",
    "# 모델이 생성한 토큰을 디코딩하여 텍스트로 변환 및 출력\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# torch is cuda available\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
